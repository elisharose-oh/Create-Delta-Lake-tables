https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/03-create-delta-tables
#Creating a Delta Lake table from a dataframe
#One of the easiest ways to create a Delta Lake table is to save a dataframe in the delta format, specifying a path where the data files and related metadata information for the table should be stored

#For example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe to a new folder location in delta format:
# Load a file into a dataframe
df = spark.read.load('/data/mydata.csv', format='csv', header=True)

# Save the dataframe as a delta table:
delta_table_path = "/delta/mydata"
df.write.format("delta").save(delta_table_path)

#You can replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode, as shown here:
new_df.write.format("delta").mode("overwrite").save(delta_table_path)

#You can also add rows from a dataframe to an existing table by using the append mode:
new_rows_df.write.format("delta").mode("append").save(delta_table_path)

#Making conditional updates

#To make such modifications to a Delta Lake table, you can use the DeltaTable object in the Delta Lake API, which supports update, delete, and merge operations
#For example, you could use the following code to update the price column for all rows with a category column value of "Accessories":

from delta.tables import *
from pyspark.sql.functions import *

# Create a deltaTable object
deltaTable = DeltaTable.forPath(spark, delta_table_path)

# Update the table (reduce price of accessories by 10%)
deltaTable.update(
    condition = "Category == 'Accessories'",
    set = { "Price": "Price * 0.9" })
    
#Querying a previous version of a table

#You can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a versionAsOf option:
df = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)

#Alternatively, you can specify a timestamp by using the timestampAsOf option:
df = spark.read.format("delta").option("timestampAsOf", '2022-01-01').load(delta_table_path)

#Create and query catalog tables

#Creating catalog tables
#You can create managed tables by writing a dataframe using the saveAsTable operation as shown in the following examples:
# Save a dataframe as a managed table
df.write.format("delta").saveAsTable("MyManagedTable")

## specify a path option to save as an external table
df.write.format("delta").option("path", "/mydata").saveAsTable("MyExternalTable")

#Creating a catalog table using SQL
#You can also create a catalog table by using the CREATE TABLE SQL statement with the USING DELTA clause, and an optional LOCATION parameter for external tables.
#You can run the statement using the SparkSQL API, like the following example:

spark.sql("CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'")

#Alternatively you can use the native SQL support in Spark to run the statement:

%sql

CREATE TABLE MyExternalTable
USING DELTA
LOCATION '/mydata'

#Defining the table schema

#when creating a new managed table, or an external table with a currently empty location, you define the table schema by specifying the column names, types, and nullability as part of the CREATE TABLE statement; as shown in the following example:

%sql

CREATE TABLE ManagedSalesOrders
(
    Orderid INT NOT NULL,
    OrderDate TIMESTAMP NOT NULL,
    CustomerName STRING,
    SalesTotal FLOAT NOT NULL
)
USING DELTA

#Using the DeltaTableBuilder API

#You can use the DeltaTableBuilder API (part of the Delta Lake API) to create a catalog table, as shown in the following example:

from delta.tables import *

DeltaTable.create(spark) \
  .tableName("default.ManagedProducts") \
  .addColumn("Productid", "INT") \
  .addColumn("ProductName", "STRING") \
  .addColumn("Category", "STRING") \
  .addColumn("Price", "FLOAT") \
  .execute()
  
#Using catalog tables

#You can use catalog tables like tables in any SQL-based relational database, querying and manipulating them by using standard SQL statements
#For example, the following code example uses a SELECT statement to query the ManagedSalesOrders table:

%sql

SELECT orderid, salestotal
FROM ManagedSalesOrders

#Use Delta Lake for streaming data

#In the following PySpark example, a Delta Lake table is used to store details of Internet sales orders
#A stream is created that reads data from the Delta Lake table folder as new data is appended.

from pyspark.sql.types import *
from pyspark.sql.functions import *

# Load a streaming dataframe from the Delta Table
stream_df = spark.readStream.format("delta") \
    .option("ignoreChanges", "true") \
    .load("/delta/internetorders")

# Now you can process the streaming data in the dataframe
# for example, show it:
stream_df.show()

#Note:When using a Delta Lake table as a streaming source, only append operations can be included in the stream
#Data modifications will cause an error unless you specify the ignoreChanges or ignoreDeletes option

#Using a Delta Lake table as a streaming sink

#In the following PySpark example, a stream of data is read from JSON files in a folder.
#The JSON data in each file contains the status for an IoT device in the format {"device":"Dev1","status":"ok"} 
#New data is added to the stream whenever a file is added to the folder.
#The input stream is a boundless dataframe, which is then written in delta format to a folder location for a Delta Lake table.

from pyspark.sql.types import *
from pyspark.sql.functions import *

# Create a stream that reads JSON data from a folder
streamFolder = '/streamingdata/'
jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
])
stream_df = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

# Write the stream to a delta table
table_path = '/delta/devicetable'
checkpoint_path = '/delta/checkpoint'
delta_stream = stream_df.writeStream.format("delta").option("checkpointLocation", checkpoint_path).start(table_path)

#Note: The checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing. 
#This file enables you to recover from failure at the point where stream processing left off.

#After the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data
#For example, the following code creates a catalog table for the Delta Lake table folder and queries it:

%sql

CREATE TABLE DeviceTable
USING DELTA
LOCATION '/delta/devicetable';

SELECT device, status
FROM DeviceTable;

#To stop the stream of data being written to the Delta Lake table, you can use the stop method of the streaming query:

delta_stream.stop()



